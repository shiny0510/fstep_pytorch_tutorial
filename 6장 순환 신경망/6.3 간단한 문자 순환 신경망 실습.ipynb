{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6.3 간단한 문자 순환 신경망 실습.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"9QGnnbwC37wF","colab_type":"code","outputId":"e33cbd6b-4d39-4802-cdcc-8b843b8d0736","executionInfo":{"status":"ok","timestamp":1559517541700,"user_tz":-540,"elapsed":4512,"user":{"displayName":"Choi Gunho","photoUrl":"","userId":"04388737836176863066"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["!pip install torch torchvision"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VwV7Yzdz4Lla","colab_type":"code","colab":{}},"source":["# 단순한 문자 RNN을 만들어보겠습니다.\n","\n","import torch \n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jcjSLaS337wU","colab_type":"code","colab":{}},"source":["# 하이퍼파라미터 설정\n","\n","n_hidden = 35 \n","lr = 0.01\n","epochs = 1000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HE0ZIXVp37wJ","colab_type":"code","colab":{}},"source":["# 사용하는 문자는 영어 소문자 및 몇가지 특수문자로 제한했습니다.\n","# alphabet(0-25), space(26), ... , start(0), end(1)\n","\n","string = \"hello pytorch. how long can a rnn cell remember? show me your limit!\"\n","chars =  \"abcdefghijklmnopqrstuvwxyz ?!.,:;01\"\n","\n","# 문자들을 리스트로 바꾸고 이의 길이(=문자의 개수)를 저장해놓습니다.\n","char_list = [i for i in chars]\n","n_letters = len(char_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DhYTZTsuGygL","colab_type":"code","colab":{}},"source":["# 문자를 그대로 쓰지않고 one-hot 벡터로 바꿔서 연산에 쓰도록 하겠습니다.\n","\n","#Start = [0 0 0 … 1 0]\n","#a =     [1 0 0 … 0 0]\n","#b =     [0 1 0 … 0 0]\n","#c =     [0 0 1 … 0 0]\n","#...\n","#end =   [0 0 0 … 0 1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0oGDc_o37wN","colab_type":"code","colab":{}},"source":["# 문자열을 one-hot 벡터의 스택으로 만드는 함수\n","# abc -> [[1 0 0 … 0 0],\n","#         [0 1 0 … 0 0],\n","#         [0 0 1 … 0 0]]\n","\n","def string_to_onehot(string):\n","    # 먼저 시작 토큰과 끝 토큰을 만들어줍니다.\n","    start = np.zeros(shape=n_letters ,dtype=int)\n","    end = np.zeros(shape=n_letters ,dtype=int)\n","    start[-2] = 1\n","    end[-1] = 1\n","    # 여기서부터는 문자열의 문자들을 차례대로 받아서 진행합니다.\n","    for i in string:\n","        # 먼저 문자가 몇번째 문자인지 찾습니다.\n","        # a:0, b:1, c:2,...\n","        idx = char_list.index(i)\n","        # 0으로만 구성된 배열을 만들어줍니다.\n","        # [0 0 0 … 0 0]\n","        zero = np.zeros(shape=n_letters ,dtype=int)\n","        # 해당 문자 인데스만 1로 바꿔줍니다.\n","        # b: [0 1 0 … 0 0]\n","        zero[idx]=1\n","        # start와 새로 생긴 zero를 붙이고 이를 start에 할당합니다.\n","        # 이게 반복되면 start에는 문자를 one-hot 벡터로 바꾼 배열들이 점점 쌓여가게 됩니다.\n","        start = np.vstack([start,zero])\n","    # 문자열이 다 끝나면 쌓아온 start와 end를 붙여줍니다.\n","    output = np.vstack([start,end])\n","    return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UNSZj7Tv37wQ","colab_type":"code","colab":{}},"source":["# One-hot 벡터를 문자로 바꿔주는 함수 \n","# [1 0 0 ... 0 0] -> a \n","# https://pytorch.org/docs/stable/tensors.html?highlight=numpy#torch.Tensor.numpy\n","\n","def onehot_to_word(onehot_1):\n","    # 텐서를 입력으로 받아 넘파이 배열로 바꿔줍니다.\n","    onehot = torch.Tensor.numpy(onehot_1)\n","    # one-hot 벡터의 최대값(=1) 위치 인덱스로 문자를 찾습니다.\n","    return char_list[onehot.argmax()]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HMI_b1MO37wR","colab_type":"code","colab":{}},"source":["# RNN with 1 hidden layer\n","\n","class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(RNN, self).__init__()\n","        \n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        \n","        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n","        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n","        self.act_fn = nn.Tanh()\n","    \n","    def forward(self, input, hidden):\n","        # 입력과 hidden state를 cat함수로 붙여줍니다.\n","        combined = torch.cat((input, hidden), 1)\n","        # 붙인 값을 i2h 및 i2o에 통과시켜 hidden state는 업데이트, 결과값은 계산해줍니다.\n","        hidden = self.act_fn(self.i2h(combined))\n","        output = self.i2o(combined)\n","        return output, hidden\n","    \n","    # 아직 입력이 없을때(t=0)의 hidden state를 초기화해줍니다. \n","    def init_hidden(self):\n","        return torch.zeros(1, self.hidden_size)\n","    \n","rnn = RNN(n_letters, n_hidden, n_letters)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dseHJCeb37wW","colab_type":"code","colab":{}},"source":["# 손실함수와 최적화함수를 설정해줍니다.\n","\n","loss_func = nn.MSELoss()\n","optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9x44BXX37wZ","colab_type":"code","outputId":"b2643cae-1f35-4c1a-8c82-bdaf2abb4acf","executionInfo":{"status":"ok","timestamp":1559517557821,"user_tz":-540,"elapsed":20539,"user":{"displayName":"Choi Gunho","photoUrl":"","userId":"04388737836176863066"}},"colab":{"base_uri":"https://localhost:8080/","height":1717}},"source":["# train\n","\n","# 문자열을 onehot 벡터로 만들고 이를 토치 텐서로 바꿔줍니다.\n","# 또한 데이터타입도 학습에 맞게 바꿔줍니다.\n","one_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())\n","\n","for i in range(epochs):\n","    optimizer.zero_grad()\n","    # 학습에 앞서 hidden state를 초기화해줍니다.\n","    hidden = rnn.init_hidden()\n","    \n","    # 문자열 전체에 대한 손실을 구하기 위해 total_loss라는 변수를 만들어줍니다. \n","    total_loss = 0\n","    for j in range(one_hot.size()[0]-1):\n","        # 입력은 앞에 글자 \n","        # pyotrch 에서 p y t o r c\n","        input_ = one_hot[j:j+1,:]\n","        # 목표값은 뒤에 글자\n","        # pytorch 에서 y t o r c h\n","        target = one_hot[j+1]\n","        output, hidden = rnn.forward(input_, hidden)\n","        \n","        loss = loss_func(output.view(-1),target.view(-1))\n","        total_loss += loss\n","\n","    total_loss.backward()\n","    optimizer.step()\n","\n","    if i % 10 == 0:\n","        print(total_loss)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(2.8275, grad_fn=<AddBackward0>)\n","tensor(1.2019, grad_fn=<AddBackward0>)\n","tensor(0.7401, grad_fn=<AddBackward0>)\n","tensor(0.4701, grad_fn=<AddBackward0>)\n","tensor(0.3155, grad_fn=<AddBackward0>)\n","tensor(0.2274, grad_fn=<AddBackward0>)\n","tensor(0.1543, grad_fn=<AddBackward0>)\n","tensor(0.1213, grad_fn=<AddBackward0>)\n","tensor(0.0939, grad_fn=<AddBackward0>)\n","tensor(0.0763, grad_fn=<AddBackward0>)\n","tensor(0.0635, grad_fn=<AddBackward0>)\n","tensor(0.0544, grad_fn=<AddBackward0>)\n","tensor(0.0472, grad_fn=<AddBackward0>)\n","tensor(0.0414, grad_fn=<AddBackward0>)\n","tensor(0.0380, grad_fn=<AddBackward0>)\n","tensor(0.0327, grad_fn=<AddBackward0>)\n","tensor(0.0354, grad_fn=<AddBackward0>)\n","tensor(0.0278, grad_fn=<AddBackward0>)\n","tensor(0.0277, grad_fn=<AddBackward0>)\n","tensor(0.0244, grad_fn=<AddBackward0>)\n","tensor(0.0217, grad_fn=<AddBackward0>)\n","tensor(0.0202, grad_fn=<AddBackward0>)\n","tensor(0.0205, grad_fn=<AddBackward0>)\n","tensor(0.0200, grad_fn=<AddBackward0>)\n","tensor(0.0174, grad_fn=<AddBackward0>)\n","tensor(0.0162, grad_fn=<AddBackward0>)\n","tensor(0.0204, grad_fn=<AddBackward0>)\n","tensor(0.0162, grad_fn=<AddBackward0>)\n","tensor(0.0143, grad_fn=<AddBackward0>)\n","tensor(0.0135, grad_fn=<AddBackward0>)\n","tensor(0.0174, grad_fn=<AddBackward0>)\n","tensor(0.0142, grad_fn=<AddBackward0>)\n","tensor(0.0126, grad_fn=<AddBackward0>)\n","tensor(0.0116, grad_fn=<AddBackward0>)\n","tensor(0.0110, grad_fn=<AddBackward0>)\n","tensor(0.0155, grad_fn=<AddBackward0>)\n","tensor(0.0136, grad_fn=<AddBackward0>)\n","tensor(0.0110, grad_fn=<AddBackward0>)\n","tensor(0.0099, grad_fn=<AddBackward0>)\n","tensor(0.0092, grad_fn=<AddBackward0>)\n","tensor(0.0110, grad_fn=<AddBackward0>)\n","tensor(0.0111, grad_fn=<AddBackward0>)\n","tensor(0.0091, grad_fn=<AddBackward0>)\n","tensor(0.0083, grad_fn=<AddBackward0>)\n","tensor(0.0077, grad_fn=<AddBackward0>)\n","tensor(0.0074, grad_fn=<AddBackward0>)\n","tensor(0.0104, grad_fn=<AddBackward0>)\n","tensor(0.0153, grad_fn=<AddBackward0>)\n","tensor(0.0089, grad_fn=<AddBackward0>)\n","tensor(0.0072, grad_fn=<AddBackward0>)\n","tensor(0.0067, grad_fn=<AddBackward0>)\n","tensor(0.0063, grad_fn=<AddBackward0>)\n","tensor(0.0060, grad_fn=<AddBackward0>)\n","tensor(0.0058, grad_fn=<AddBackward0>)\n","tensor(0.0071, grad_fn=<AddBackward0>)\n","tensor(0.0083, grad_fn=<AddBackward0>)\n","tensor(0.0067, grad_fn=<AddBackward0>)\n","tensor(0.0054, grad_fn=<AddBackward0>)\n","tensor(0.0052, grad_fn=<AddBackward0>)\n","tensor(0.0050, grad_fn=<AddBackward0>)\n","tensor(0.0150, grad_fn=<AddBackward0>)\n","tensor(0.0064, grad_fn=<AddBackward0>)\n","tensor(0.0051, grad_fn=<AddBackward0>)\n","tensor(0.0046, grad_fn=<AddBackward0>)\n","tensor(0.0044, grad_fn=<AddBackward0>)\n","tensor(0.0136, grad_fn=<AddBackward0>)\n","tensor(0.0054, grad_fn=<AddBackward0>)\n","tensor(0.0046, grad_fn=<AddBackward0>)\n","tensor(0.0041, grad_fn=<AddBackward0>)\n","tensor(0.0038, grad_fn=<AddBackward0>)\n","tensor(0.0037, grad_fn=<AddBackward0>)\n","tensor(0.0228, grad_fn=<AddBackward0>)\n","tensor(0.0074, grad_fn=<AddBackward0>)\n","tensor(0.0049, grad_fn=<AddBackward0>)\n","tensor(0.0039, grad_fn=<AddBackward0>)\n","tensor(0.0035, grad_fn=<AddBackward0>)\n","tensor(0.0033, grad_fn=<AddBackward0>)\n","tensor(0.0032, grad_fn=<AddBackward0>)\n","tensor(0.0204, grad_fn=<AddBackward0>)\n","tensor(0.0057, grad_fn=<AddBackward0>)\n","tensor(0.0037, grad_fn=<AddBackward0>)\n","tensor(0.0033, grad_fn=<AddBackward0>)\n","tensor(0.0031, grad_fn=<AddBackward0>)\n","tensor(0.0029, grad_fn=<AddBackward0>)\n","tensor(0.0028, grad_fn=<AddBackward0>)\n","tensor(0.0028, grad_fn=<AddBackward0>)\n","tensor(0.0072, grad_fn=<AddBackward0>)\n","tensor(0.0047, grad_fn=<AddBackward0>)\n","tensor(0.0036, grad_fn=<AddBackward0>)\n","tensor(0.0029, grad_fn=<AddBackward0>)\n","tensor(0.0026, grad_fn=<AddBackward0>)\n","tensor(0.0025, grad_fn=<AddBackward0>)\n","tensor(0.0024, grad_fn=<AddBackward0>)\n","tensor(0.0023, grad_fn=<AddBackward0>)\n","tensor(0.0186, grad_fn=<AddBackward0>)\n","tensor(0.0050, grad_fn=<AddBackward0>)\n","tensor(0.0030, grad_fn=<AddBackward0>)\n","tensor(0.0026, grad_fn=<AddBackward0>)\n","tensor(0.0023, grad_fn=<AddBackward0>)\n","tensor(0.0022, grad_fn=<AddBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O8suWP0r37wf","colab_type":"code","outputId":"891bbd66-520f-4434-f502-d91aee67ce0a","executionInfo":{"status":"ok","timestamp":1559517557823,"user_tz":-540,"elapsed":20534,"user":{"displayName":"Choi Gunho","photoUrl":"","userId":"04388737836176863066"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# test \n","# hidden state 는 처음 한번만 초기화해줍니다.\n","\n","start = torch.zeros(1,n_letters)\n","start[:,-2] = 1\n","\n","with torch.no_grad():\n","    hidden = rnn.init_hidden()\n","    # 처음 입력으로 start token을 전달해줍니다.\n","    input_ = start\n","    # output string에 문자들을 계속 붙여줍니다.\n","    output_string = \"\"\n","\n","    # 원래는 end token이 나올때 까지 반복하는게 맞으나 끝나지 않아서 string의 길이로 정했습니다.\n","    for i in range(len(string)):\n","        output, hidden = rnn.forward(input_, hidden)\n","        # 결과값을 문자로 바꿔서 output_string에 붙여줍니다.\n","        output_string += onehot_to_word(output.data)\n","        # 또한 이번의 결과값이 다음의 입력값이 됩니다.\n","        input_ = output\n","\n","print(output_string)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["hello pytorch ml longeomm rome om  omb ome om eomg omelo. eongnomelp\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4Oks8JryMcCG","colab_type":"text"},"source":["## 이 파일을 다 보셨으면 [참고]LSTM 연습코드를 먼저 보시고 6.4절로 넘어가시는걸 추천드립니다."]}]}